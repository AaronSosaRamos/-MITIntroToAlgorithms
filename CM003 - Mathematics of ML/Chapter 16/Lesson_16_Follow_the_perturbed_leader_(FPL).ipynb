{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FOLLOW THE PERTURBED LEADER (FPL)"
      ],
      "metadata": {
        "id": "X0xPb2_4R1iZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcGpdDtHRlq3",
        "outputId": "a17f19cd-4160-4cba-f157-9bc77b973bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights: [6.75582420695102e+54, 6.75582420695102e+54]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "class FollowThePerturbedLeader:\n",
        "    def __init__(self, num_actions, eta):\n",
        "        self.num_actions = num_actions  # Number of actions (arms)\n",
        "        self.eta = eta  # Exploration parameter\n",
        "\n",
        "        # Initialize weights for each action\n",
        "        self.weights = [1.0] * num_actions\n",
        "\n",
        "    def choose_action(self):\n",
        "        # Perturb weights with random noise\n",
        "        perturbed_weights = [w + self.eta * random.random() for w in self.weights]\n",
        "\n",
        "        # Choose action with the highest perturbed weight\n",
        "        chosen_action = max(range(self.num_actions), key=lambda i: perturbed_weights[i])\n",
        "\n",
        "        return chosen_action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        # Update weights based on received reward\n",
        "        total_weight = sum(self.weights)\n",
        "        normalized_rewards = [reward * (w / total_weight) for w in self.weights]\n",
        "\n",
        "        for i in range(self.num_actions):\n",
        "            self.weights[i] *= math.exp(normalized_rewards[i] / self.num_actions)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage for a two-armed bandit\n",
        "    num_actions = 2\n",
        "    eta = 1.0  # Exploration parameter\n",
        "\n",
        "    fpl = FollowThePerturbedLeader(num_actions, eta)\n",
        "\n",
        "    # Simulation of bandit game\n",
        "    num_steps = 1000\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        action = fpl.choose_action()\n",
        "\n",
        "        # Simulate reward (0 or 1 for simplicity)\n",
        "        reward = random.randint(0, 1)\n",
        "\n",
        "        # Update FPL algorithm with the observed reward\n",
        "        fpl.update(action, reward)\n",
        "\n",
        "        # Collect reward for plotting or analysis\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Print final weights (optional)\n",
        "    print(\"Final weights:\", fpl.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exponential Weights Algorithm (Exp3)"
      ],
      "metadata": {
        "id": "JQOp2BvsfgK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Exponential Weights Algorithm (Exp3) is a well-known algorithm for multi-armed bandit problems, providing a good balance between exploration and exploitation."
      ],
      "metadata": {
        "id": "LKLYj4vlgF-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "class Exp3:\n",
        "    def __init__(self, num_actions, gamma):\n",
        "        self.num_actions = num_actions\n",
        "        self.gamma = gamma  # Exploration parameter\n",
        "\n",
        "        self.weights = [1.0] * num_actions\n",
        "\n",
        "    def choose_action(self):\n",
        "        total_weight = sum(self.weights)\n",
        "        probabilities = [((1 - self.gamma) * (w / total_weight)) + (self.gamma / self.num_actions) for w in self.weights]\n",
        "\n",
        "        chosen_action = random.choices(range(self.num_actions), probabilities)[0]\n",
        "        return chosen_action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        total_weight = sum(self.weights)\n",
        "        estimated_reward = reward / max(1e-5, ((1 - self.gamma) * (self.weights[action] / total_weight)) + (self.gamma / self.num_actions))\n",
        "        self.weights[action] *= math.exp(self.gamma * estimated_reward / self.num_actions)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_actions = 2\n",
        "    gamma = 0.1\n",
        "\n",
        "    exp3 = Exp3(num_actions, gamma)\n",
        "\n",
        "    num_steps = 1000\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        action = exp3.choose_action()\n",
        "        reward = random.randint(0, 1)\n",
        "        exp3.update(action, reward)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    print(\"Final weights:\", exp3.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xTdd998fglT",
        "outputId": "206e69fe-3167-43eb-ecf2-71b7662487d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights: [603842886079.1451, 253037565037.36798]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Softmax Exploration"
      ],
      "metadata": {
        "id": "ojLMxhSUfrqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Exploration is another technique used in bandit problems that utilizes a softmax function to convert action values into probabities.\n",
        "\n"
      ],
      "metadata": {
        "id": "ejSN49t0gIC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "class SoftmaxExplorer:\n",
        "    def __init__(self, num_actions, tau):\n",
        "        self.num_actions = num_actions\n",
        "        self.tau = tau  # Temperature parameter\n",
        "\n",
        "        self.action_values = [0.0] * num_actions\n",
        "\n",
        "    def choose_action(self):\n",
        "        # Apply a stable softmax calculation using logarithmic transformation\n",
        "        log_values = [value / self.tau for value in self.action_values]\n",
        "        max_log_value = max(log_values)\n",
        "\n",
        "        # Compute softmax probabilities in log space to prevent overflow\n",
        "        exp_values = [math.exp(log_value - max_log_value) for log_value in log_values]\n",
        "        total_exp = sum(exp_values)\n",
        "        probabilities = [value / total_exp for value in exp_values]\n",
        "\n",
        "        # Choose action using the computed probabilities\n",
        "        chosen_action = random.choices(range(self.num_actions), probabilities)[0]\n",
        "        return chosen_action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        # Simple update: increment the value of the chosen action by the reward received\n",
        "        self.action_values[action] += reward\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_actions = 2\n",
        "    tau = 0.5\n",
        "\n",
        "    softmax_explorer = SoftmaxExplorer(num_actions, tau)\n",
        "\n",
        "    num_steps = 1000\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        action = softmax_explorer.choose_action()\n",
        "        reward = random.randint(0, 1)\n",
        "        softmax_explorer.update(action, reward)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    print(\"Final action values:\", softmax_explorer.action_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-7vT8WzfsC8",
        "outputId": "d1368445-36d6-485d-bd11-4ddb9840be14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final action values: [0.0, 521.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UCB1 Algorithm"
      ],
      "metadata": {
        "id": "90DxZg2ugBcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Upper Confidence Bound (UCB1) algorithm is used for exploration-exploitation trade-off by selecting actions based on upper confidence bounds of their estimated rewards."
      ],
      "metadata": {
        "id": "LkvND4lYgL1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "class UCB1:\n",
        "    def __init__(self, num_actions):\n",
        "        self.num_actions = num_actions\n",
        "        self.action_counts = [0] * num_actions\n",
        "        self.action_values = [0.0] * num_actions\n",
        "        self.timestep = 0\n",
        "\n",
        "    def choose_action(self):\n",
        "        if 0 in self.action_counts:\n",
        "            return self.action_counts.index(0)  # Choose unexplored action\n",
        "\n",
        "        ucb_values = [value + math.sqrt(2 * math.log(self.timestep) / count) for value, count in zip(self.action_values, self.action_counts)]\n",
        "        chosen_action = ucb_values.index(max(ucb_values))\n",
        "        return chosen_action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        self.timestep += 1\n",
        "        self.action_counts[action] += 1\n",
        "        self.action_values[action] += (reward - self.action_values[action]) / self.action_counts[action]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_actions = 2\n",
        "\n",
        "    ucb1 = UCB1(num_actions)\n",
        "\n",
        "    num_steps = 1000\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        action = ucb1.choose_action()\n",
        "        reward = random.randint(0, 1)\n",
        "        ucb1.update(action, reward)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    print(\"Final action values:\", ucb1.action_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luuqJv58gCU5",
        "outputId": "07c621d9-61f5-464f-d7ca-50cf5efdb951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final action values: [0.4980392156862743, 0.49591836734693884]\n"
          ]
        }
      ]
    }
  ]
}