{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Projected Gradient Descent"
      ],
      "metadata": {
        "id": "ebj5HzORqrR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "B0f_6EUrqyNK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmDObsf6qf9s",
        "outputId": "5af35da6-b6c5-4a6d-aaec-e322144623b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal solution: [2.54629497e-10 8.15134630e-11]\n",
            "Optimal value of f(x): 7.148062540804482e-20\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def projected_gradient_descent(f, grad_f, project, x0, lr=0.01, num_steps=100):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) for minimizing f(x) subject to the constraint x in C.\n",
        "\n",
        "    Parameters:\n",
        "        f (function): Objective function to minimize.\n",
        "        grad_f (function): Gradient function of f.\n",
        "        project (function): Projection function onto the feasible set C.\n",
        "        x0 (numpy array): Initial point.\n",
        "        lr (float): Learning rate or step size for gradient descent.\n",
        "        num_steps (int): Number of PGD steps.\n",
        "\n",
        "    Returns:\n",
        "        numpy array: The optimized point x.\n",
        "    \"\"\"\n",
        "    x = x0.copy()\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        gradient = grad_f(x)\n",
        "        x = x - lr * gradient  # Gradient descent step\n",
        "        x = project(x)         # Projection onto the feasible set C\n",
        "\n",
        "    return x\n",
        "\n",
        "# Example usage:\n",
        "# Define the objective function and its gradient\n",
        "def quadratic(x):\n",
        "    return np.sum(x**2)\n",
        "\n",
        "def grad_quadratic(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Define the projection function onto a hypercube [a, b]\n",
        "def project_hypercube(x, a=-1, b=1):\n",
        "    return np.maximum(a, np.minimum(b, x))\n",
        "\n",
        "# Set up parameters\n",
        "np.random.seed(0)\n",
        "x0 = np.random.randn(2)  # Initial point\n",
        "lr = 0.1                 # Learning rate\n",
        "num_steps = 100          # Number of PGD steps\n",
        "\n",
        "# Run Projected Gradient Descent\n",
        "x_opt = projected_gradient_descent(quadratic, grad_quadratic, project_hypercube, x0, lr=lr, num_steps=num_steps)\n",
        "\n",
        "print(\"Optimal solution:\", x_opt)\n",
        "print(\"Optimal value of f(x):\", quadratic(x_opt))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Projection onto Closed Convex Set"
      ],
      "metadata": {
        "id": "5vSIuU5tq_9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def projection_onto_convex_set(x0, constraints):\n",
        "    \"\"\"\n",
        "    Project a point x0 onto a closed convex set defined by linear inequality constraints.\n",
        "\n",
        "    Parameters:\n",
        "        x0 (numpy array): The point to project onto the convex set.\n",
        "        constraints (list of dict): List of constraint dictionaries, each specifying a linear inequality constraint.\n",
        "                                     Each dictionary has keys 'type', 'fun', and 'jac'.\n",
        "                                     'type': Constraint type ('ineq' for inequality).\n",
        "                                     'fun': Function defining the constraint (returns >= 0 for feasibility).\n",
        "                                     'jac': Jacobian (gradient) of the constraint function.\n",
        "\n",
        "    Returns:\n",
        "        numpy array: The projected point onto the convex set.\n",
        "    \"\"\"\n",
        "    # Define an objective function to minimize (distance from x0)\n",
        "    def objective(y):\n",
        "        return np.linalg.norm(y - x0)**2\n",
        "\n",
        "    # Initialize optimization bounds (unconstrained)\n",
        "    bounds = [(None, None) for _ in range(len(x0))]\n",
        "\n",
        "    # Use scipy.optimize.minimize to perform the projection\n",
        "    result = minimize(objective, x0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "\n",
        "    # Return the optimized point (projected point)\n",
        "    return result.x\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the point x0 and constraints for a convex set (e.g., hypercube)\n",
        "    x0 = np.array([1.5, 2.0, -1.0])\n",
        "\n",
        "    # Define linear inequality constraints for a hypercube [a, b]^n (e.g., [-1, 1]^3)\n",
        "    # Constraints: -1 <= x_i <= 1 for all i\n",
        "    constraints = [\n",
        "        {'type': 'ineq', 'fun': lambda x: x[0] - (-1), 'jac': lambda x: np.array([1.0, 0.0, 0.0])},  # x1 >= -1\n",
        "        {'type': 'ineq', 'fun': lambda x: 1 - x[0], 'jac': lambda x: np.array([-1.0, 0.0, 0.0])},   # x1 <= 1\n",
        "        {'type': 'ineq', 'fun': lambda x: x[1] - (-1), 'jac': lambda x: np.array([0.0, 1.0, 0.0])},  # x2 >= -1\n",
        "        {'type': 'ineq', 'fun': lambda x: 1 - x[1], 'jac': lambda x: np.array([0.0, -1.0, 0.0])},   # x2 <= 1\n",
        "        {'type': 'ineq', 'fun': lambda x: x[2] - (-1), 'jac': lambda x: np.array([0.0, 0.0, 1.0])},  # x3 >= -1\n",
        "        {'type': 'ineq', 'fun': lambda x: 1 - x[2], 'jac': lambda x: np.array([0.0, 0.0, -1.0])}    # x3 <= 1\n",
        "    ]\n",
        "\n",
        "    # Project x0 onto the hypercube [-1, 1]^3\n",
        "    projected_x = projection_onto_convex_set(x0, constraints)\n",
        "\n",
        "    print(\"Original point x0:\", x0)\n",
        "    print(\"Projected point onto the hypercube [-1, 1]^3:\", projected_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIReqXbjrQNH",
        "outputId": "934a07b4-1acc-400e-cb50-c8e8c2aa20d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original point x0: [ 1.5  2.  -1. ]\n",
            "Projected point onto the hypercube [-1, 1]^3: [ 1.  1. -1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Projected Gradient Descent"
      ],
      "metadata": {
        "id": "neTSftCNrVqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def projected_gradient_descent(f, grad_f, project, x0, lr=0.01, num_steps=100, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) for minimizing f(x) subject to the constraint x in C.\n",
        "\n",
        "    Parameters:\n",
        "        f (function): Objective function to minimize.\n",
        "        grad_f (function): Gradient function of f.\n",
        "        project (function): Projection function onto the feasible set C.\n",
        "        x0 (numpy array): Initial point.\n",
        "        lr (float): Learning rate or step size for gradient descent.\n",
        "        num_steps (int): Number of PGD steps.\n",
        "        tol (float): Tolerance to stop optimization (based on change in x).\n",
        "\n",
        "    Returns:\n",
        "        numpy array: The optimized point x.\n",
        "    \"\"\"\n",
        "    x = x0.copy()\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        gradient = grad_f(x)\n",
        "        x_new = x - lr * gradient  # Gradient descent step\n",
        "        x_new = project(x_new)     # Projection onto the feasible set C\n",
        "\n",
        "        # Check convergence based on change in x\n",
        "        if np.linalg.norm(x_new - x) < tol:\n",
        "            break\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    return x\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the objective function and its gradient\n",
        "    def quadratic(x):\n",
        "        return np.sum(x**2)\n",
        "\n",
        "    def grad_quadratic(x):\n",
        "        return 2 * x\n",
        "\n",
        "    # Define the projection function onto a hypercube [a, b]\n",
        "    def project_hypercube(x, a=-1, b=1):\n",
        "        return np.maximum(a, np.minimum(b, x))\n",
        "\n",
        "    # Set up parameters\n",
        "    np.random.seed(0)\n",
        "    x0 = np.random.randn(2)  # Initial point\n",
        "    lr = 0.1                 # Learning rate\n",
        "    num_steps = 100          # Number of PGD steps\n",
        "    tol = 1e-6               # Tolerance for stopping criterion\n",
        "\n",
        "    # Run Projected Gradient Descent\n",
        "    x_opt = projected_gradient_descent(quadratic, grad_quadratic, project_hypercube, x0, lr=lr, num_steps=num_steps, tol=tol)\n",
        "\n",
        "    print(\"Optimal solution:\", x_opt)\n",
        "    print(\"Optimal value of f(x):\", quadratic(x_opt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5HGWXaJrWXX",
        "outputId": "04fbeb0d-7723-4426-98d2-7ee8ef7390e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal solution: [4.67680524e-06 1.49716586e-06]\n",
            "Optimal value of f(x): 2.411401286904067e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
        "\n",
        "# Initialize SVM model\n",
        "svm = SVC(kernel='linear', C=1.0)\n",
        "\n",
        "# Fit SVM model to the data\n",
        "svm.fit(X, y)\n",
        "\n",
        "# Get the learned parameters\n",
        "w_svm = np.concatenate([svm.coef_.flatten(), [svm.intercept_[0]]])\n",
        "\n",
        "# Define the hinge loss function for SVM\n",
        "def hinge_loss(w, X, y, C=1.0):\n",
        "    margin = y * (np.dot(X, w[:-1]) + w[-1])\n",
        "    loss = np.maximum(0, 1 - margin)\n",
        "    return C * np.mean(loss)\n",
        "\n",
        "# Define the gradient of the hinge loss function for SVM\n",
        "def grad_hinge_loss(w, X, y, C=1.0):\n",
        "    margin = y * (np.dot(X, w[:-1]) + w[-1])\n",
        "    indicator = (margin < 1).astype(int)\n",
        "    grad = np.zeros_like(w)\n",
        "    grad[:-1] = -C * np.mean((X.T * y * indicator).T, axis=0)\n",
        "    grad[-1] = -C * np.mean(y * indicator)\n",
        "    return grad\n",
        "\n",
        "# Projected Gradient Descent function for SVM\n",
        "def projected_gradient_descent_svm(X, y, C=1.0, lr=0.01, num_steps=100):\n",
        "    w = np.zeros(X.shape[1] + 1)  # Initialize weights (including bias)\n",
        "    for _ in range(num_steps):\n",
        "        gradient = grad_hinge_loss(w, X, y, C=C)\n",
        "        w_new = w - lr * gradient  # Gradient descent step\n",
        "        # Projection onto the feasible set (L2 ball for SVM)\n",
        "        norm = np.linalg.norm(w_new[:-1])\n",
        "        if norm > np.sqrt(C):\n",
        "            w_new[:-1] = w_new[:-1] * np.sqrt(C) / norm\n",
        "        w = w_new\n",
        "    return w\n",
        "\n",
        "# Run Projected Gradient Descent for SVM\n",
        "w_svm_pgd = projected_gradient_descent_svm(X, y)\n",
        "\n",
        "print(\"Weights learned by SVM:\", w_svm)\n",
        "print(\"Weights learned by PGD for SVM:\", w_svm_pgd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gZVmb1_slbZ",
        "outputId": "840a0cdf-fd99-46c9-8070-aa1f09f7a0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights learned by SVM: [ 2.36795628 -0.72852864  0.08829376]\n",
            "Weights learned by PGD for SVM: [4.59693294e-01 3.37845861e-04 4.49200000e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Generate synthetic data\n",
        "X_boost, y_boost = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
        "\n",
        "# Initialize Boosting model\n",
        "boosting = DecisionTreeRegressor(max_depth=1)\n",
        "\n",
        "# Fit the first weak learner (initial constant prediction)\n",
        "boosting.fit(X_boost, y_boost)\n",
        "\n",
        "# Define the exponential loss function for Boosting\n",
        "def exponential_loss(y_true, y_pred):\n",
        "    return np.mean(np.exp(-y_true * y_pred))\n",
        "\n",
        "# Define the gradient of the exponential loss function for Boosting\n",
        "def grad_exponential_loss(y_true, y_pred):\n",
        "    return -y_true * np.exp(-y_true * y_pred)\n",
        "\n",
        "# Projected Gradient Descent function for Boosting\n",
        "def projected_gradient_descent_boosting(X, y, lr=0.1, num_steps=100):\n",
        "    predictions = np.zeros(len(y))  # Initialize predictions (initially zeros)\n",
        "    for _ in range(num_steps):\n",
        "        # Compute the gradient of the loss w.r.t. current predictions\n",
        "        gradient = grad_exponential_loss(y, predictions)\n",
        "        # Update the predictions using the gradient\n",
        "        predictions -= lr * gradient\n",
        "        # Project the predictions onto the feasible set (range of y)\n",
        "        predictions = np.maximum(-1, np.minimum(1, predictions))\n",
        "    return predictions\n",
        "\n",
        "# Run Projected Gradient Descent for Boosting\n",
        "predictions_boosting_pgd = projected_gradient_descent_boosting(X_boost, y_boost)\n",
        "\n",
        "# Compute the coefficients based on the final predictions (for demonstration purposes)\n",
        "# Handle division by zero gracefully\n",
        "predictions_boosting_pgd = np.clip(predictions_boosting_pgd, -0.999999, 0.999999)\n",
        "coeffs_boosting_pgd = 0.5 * np.log((1 + predictions_boosting_pgd) / (1 - predictions_boosting_pgd))\n",
        "\n",
        "print(\"Coefficients learned by PGD for Boosting:\", coeffs_boosting_pgd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRoLJqtxsp2q",
        "outputId": "ddec177a-c4d4-47e7-fe9d-5d3894d562ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients learned by PGD for Boosting: [0.         7.25432862 0.         0.         7.25432862 0.\n",
            " 7.25432862 7.25432862 0.         7.25432862 7.25432862 0.\n",
            " 0.         7.25432862 7.25432862 0.         7.25432862 0.\n",
            " 7.25432862 0.         7.25432862 7.25432862 0.         0.\n",
            " 7.25432862 0.         7.25432862 7.25432862 7.25432862 7.25432862\n",
            " 0.         0.         0.         0.         7.25432862 7.25432862\n",
            " 0.         0.         0.         0.         7.25432862 7.25432862\n",
            " 0.         7.25432862 7.25432862 0.         7.25432862 7.25432862\n",
            " 7.25432862 7.25432862 0.         7.25432862 7.25432862 7.25432862\n",
            " 0.         0.         0.         7.25432862 0.         0.\n",
            " 0.         7.25432862 7.25432862 7.25432862 0.         7.25432862\n",
            " 0.         0.         0.         7.25432862 7.25432862 0.\n",
            " 0.         7.25432862 7.25432862 0.         7.25432862 7.25432862\n",
            " 0.         0.         0.         7.25432862 0.         0.\n",
            " 7.25432862 7.25432862 7.25432862 7.25432862 0.         0.\n",
            " 0.         0.         7.25432862 0.         7.25432862 7.25432862\n",
            " 7.25432862 0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mirror Descent with  Bregman Projections"
      ],
      "metadata": {
        "id": "oC1jOr2pryxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mirror_descent(f, grad_f, bregman_div, bregman_prox, x0, lr=0.1, num_steps=100):\n",
        "    \"\"\"\n",
        "    Mirror Descent with Bregman Projections for minimizing f(x) subject to constraints defined by a Bregman divergence.\n",
        "\n",
        "    Parameters:\n",
        "        f (function): Objective function to minimize.\n",
        "        grad_f (function): Gradient function of f.\n",
        "        bregman_div (function): Bregman divergence function.\n",
        "        bregman_prox (function): Bregman projection function.\n",
        "        x0 (numpy array): Initial point.\n",
        "        lr (float): Learning rate or step size for Mirror Descent.\n",
        "        num_steps (int): Number of Mirror Descent steps.\n",
        "\n",
        "    Returns:\n",
        "        numpy array: The optimized point x.\n",
        "    \"\"\"\n",
        "    x = x0.copy()\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        gradient = grad_f(x)\n",
        "        x_new = x - lr * gradient  # Gradient descent step\n",
        "        x_new = bregman_prox(x_new)  # Bregman projection onto the feasible set\n",
        "        x = x_new\n",
        "\n",
        "    return x\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the objective function and its gradient\n",
        "    def quadratic(x):\n",
        "        return np.sum(x**2)\n",
        "\n",
        "    def grad_quadratic(x):\n",
        "        return 2 * x\n",
        "\n",
        "    # Define the Bregman divergence (squared Euclidean distance)\n",
        "    def squared_euclidean_divergence(x, y):\n",
        "        return np.sum((x - y)**2)\n",
        "\n",
        "    # Define the Bregman projection onto the Euclidean ball (L2 norm ball)\n",
        "    def euclidean_projection(x, center=np.zeros(2), radius=1.0):\n",
        "        direction = x - center\n",
        "        norm = np.linalg.norm(direction)\n",
        "        if norm > radius:\n",
        "            return center + (radius / norm) * direction\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    # Set up parameters\n",
        "    np.random.seed(0)\n",
        "    x0 = np.random.randn(2)  # Initial point\n",
        "    lr = 0.1                 # Learning rate\n",
        "    num_steps = 100          # Number of Mirror Descent steps\n",
        "\n",
        "    # Run Mirror Descent with Bregman Projections\n",
        "    x_opt = mirror_descent(quadratic, grad_quadratic, squared_euclidean_divergence, euclidean_projection, x0, lr=lr, num_steps=num_steps)\n",
        "\n",
        "    print(\"Optimal solution:\", x_opt)\n",
        "    print(\"Optimal value of f(x):\", quadratic(x_opt))"
      ],
      "metadata": {
        "id": "aSyxJHuXuqZK",
        "outputId": "4ff756b1-202c-481e-a92f-f65648644591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal solution: [2.48320803e-10 5.63290309e-11]\n",
            "Optimal value of f(x): 6.483618076376559e-20\n"
          ]
        }
      ]
    }
  ]
}