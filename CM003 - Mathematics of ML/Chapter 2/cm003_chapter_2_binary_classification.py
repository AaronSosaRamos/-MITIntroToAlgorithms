# -*- coding: utf-8 -*-
"""CM003 - Chapter 2 - Binary Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QmHvoRnPSAt4tdePkRagghS3eNn6xA4C

#BINARY CLASSIFICATION
"""

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Consider only the first two classes (Setosa: 0, Versicolor: 1)
X = X[y != 2]
y = y[y != 2]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define a list of classifiers
classifiers = [
    LogisticRegression(),
    DecisionTreeClassifier(),
    SVC(),
    KNeighborsClassifier(),
    RandomForestClassifier(),
    GradientBoostingClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis()
]

# Evaluate each classifier
results = {}
for clf in classifiers:
    clf_name = clf.__class__.__name__
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results[clf_name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-score': f1
    }

    print(f"Classifier: {clf_name}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-score: {f1:.4f}")
    print("="*30)

# Define metrics and corresponding titles for plots
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']
titles = ['Accuracy Scores', 'Precision Scores', 'Recall Scores', 'F1-scores']

# Plot results for each metric
for metric, title in zip(metrics, titles):
    plt.figure(figsize=(12, 8))
    for clf_name in results:
        score = results[clf_name][metric]
        plt.bar(clf_name, score, alpha=0.8, label=clf_name)

    plt.xlabel('Classifier')
    plt.ylabel(metric)
    plt.title(title)
    plt.xticks(rotation=45)
    plt.legend(loc='lower right')
    plt.tight_layout()
    plt.show()

"""#Bayes Classifier"""

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.naive_bayes import GaussianNB, BernoulliNB

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Consider only the first two classes (Setosa: 0, Versicolor: 1)
X = X[y != 2]
y = y[y != 2]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define a list of compatible Naive Bayes classifiers
classifiers = [
    GaussianNB(),
    BernoulliNB()
]

# Evaluate each Naive Bayes classifier
results = {}
for clf in classifiers:
    clf_name = clf.__class__.__name__
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results[clf_name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-score': f1
    }

    print(f"Classifier: {clf_name}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-score: {f1:.4f}")
    print("="*30)

# Define metrics and corresponding titles for plots
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']
titles = ['Accuracy Scores', 'Precision Scores', 'Recall Scores', 'F1-scores']

# Plot results for each metric
for metric, title in zip(metrics, titles):
    plt.figure(figsize=(12, 8))
    for clf_name in results:
        score = results[clf_name][metric]
        plt.bar(clf_name, score, alpha=0.8, label=clf_name)

    plt.xlabel('Classifier')
    plt.ylabel(metric)
    plt.title(title)
    plt.xticks(rotation=45)
    plt.legend(loc='lower right')
    plt.tight_layout()
    plt.show()

"""#Empirical Risk Minimization"""

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.naive_bayes import GaussianNB, BernoulliNB

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Consider only the first two classes (Setosa: 0, Versicolor: 1)
X = X[y != 2]
y = y[y != 2]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define Bayesian classifiers (GaussianNB and BernoulliNB)
classifiers = [
    GaussianNB(),
    BernoulliNB()
]

# Train and evaluate each Bayesian classifier using Empirical Risk Minimization
results = {}
for clf in classifiers:
    clf_name = clf.__class__.__name__

    # Fit the classifier on the training data
    clf.fit(X_train, y_train)

    # Predict on the test data
    y_pred = clf.predict(X_test)

    # Compute performance metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Store the results
    results[clf_name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-score': f1
    }

    # Print performance metrics for the classifier
    print(f"Classifier: {clf_name}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-score: {f1:.4f}")
    print("=" * 30)

# Define metrics and corresponding titles for plots
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']
titles = ['Accuracy Scores', 'Precision Scores', 'Recall Scores', 'F1-scores']

# Plot results for each metric
for metric, title in zip(metrics, titles):
    plt.figure(figsize=(12, 8))
    for clf_name in results:
        score = results[clf_name][metric]
        plt.bar(clf_name, score, alpha=0.8, label=clf_name)

    plt.xlabel('Classifier')
    plt.ylabel(metric)
    plt.title(title)
    plt.xticks(rotation=45)
    plt.legend(loc='lower right')
    plt.tight_layout()
    plt.show()

"""#Oracle Inequalities"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.linspace(0, 10, n_samples)[:, np.newaxis]
y = np.sin(X).ravel() + np.random.normal(0, 0.3, n_samples)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define polynomial degrees to evaluate
degrees = [1, 3, 5, 9]

# Initialize lists to store results
train_errors = []
test_errors = []

# Iterate over polynomial degrees
for degree in degrees:
    # Create polynomial features
    polynomial_features = PolynomialFeatures(degree=degree)
    X_poly_train = polynomial_features.fit_transform(X_train)
    X_poly_test = polynomial_features.transform(X_test)

    # Fit linear regression model
    model = LinearRegression()
    model.fit(X_poly_train, y_train)

    # Predict on training and test data
    y_train_pred = model.predict(X_poly_train)
    y_test_pred = model.predict(X_poly_test)

    # Calculate mean squared errors (empirical risk)
    train_error = mean_squared_error(y_train, y_train_pred)
    test_error = mean_squared_error(y_test, y_test_pred)

    # Store errors
    train_errors.append(train_error)
    test_errors.append(test_error)

# Plot training and test errors vs. model complexity (degree)
plt.figure(figsize=(10, 6))
plt.plot(degrees, train_errors, label='Train Error', marker='o')
plt.plot(degrees, test_errors, label='Test Error', marker='o')
plt.xlabel('Polynomial Degree')
plt.ylabel('Mean Squared Error')
plt.title('Model Complexity vs. Empirical Risk')
plt.xticks(degrees)
plt.legend()
plt.grid(True)
plt.show()

"""#Hoeffdingâ€™s Theorem"""

import numpy as np
import matplotlib.pyplot as plt

# Function to simulate flipping a biased coin
def flip_coin(p, n):
    """Simulate flipping a biased coin with probability p."""
    return np.random.binomial(1, p, n)

# Function to calculate empirical mean
def empirical_mean(X):
    """Calculate empirical mean of a sequence X."""
    return np.mean(X)

# Hoeffding's inequality
def hoeffding_inequality(epsilon, n):
    """Compute the upper bound probability using Hoeffding's inequality."""
    return 2 * np.exp(-2 * epsilon**2 * n)

# Parameters
true_mean = 0.3  # True probability of heads (biased coin)
n = 1000         # Number of coin flips
epsilon_values = np.linspace(0, 0.5, n)  # Range of epsilon values matching number of flips

# Simulate coin flips
coin_flips = flip_coin(true_mean, n)

# Calculate empirical means
empirical_means = [empirical_mean(coin_flips[:i+1]) for i in range(n)]

# Compute Hoeffding's bounds for different epsilon values
hoeffding_bounds = [hoeffding_inequality(epsilon, n) for epsilon in epsilon_values]

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(range(1, n+1), empirical_means, label='Empirical Mean')
plt.plot([1, n], [true_mean, true_mean], 'r--', label='True Mean')
plt.fill_between(range(1, n+1), true_mean - epsilon_values, true_mean + epsilon_values,
                 alpha=0.3, label='True Mean $\pm \epsilon$')
plt.xlabel('Number of Coin Flips')
plt.ylabel('Empirical Mean')
plt.title('Empirical Mean vs. True Mean with Hoeffding\'s Inequality')
plt.legend()
plt.grid(True)
plt.show()

# Print Hoeffding's bound for epsilon = 0.1
epsilon = 0.1
bound = hoeffding_inequality(epsilon, n)
print(f"Hoeffding's Bound for epsilon = {epsilon}: {bound:.4f}")