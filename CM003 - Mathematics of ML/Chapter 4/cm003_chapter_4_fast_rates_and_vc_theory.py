# -*- coding: utf-8 -*-
"""CM003 - Chapter 4 - Fast Rates and VC Theory.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGX0pu9awtzKYjD7hHziuKms1-MijnZG

#Fast Rates and VC Theory

# Noise conditions
"""

import math

def excess_risk_bound(M, δ, n, γ):
    """
    Calculate the excess risk bound based on Massart's noise condition.

    Args:
        M (int): Number of classifiers.
        δ (float): Probability threshold.
        n (int): Number of samples.
        γ (float): Constant γ satisfying Massart's noise condition.

    Returns:
        float: Excess risk bound.
    """
    log_term = math.log(M / δ)
    excess_risk_bound = (log_term) / (γ * n)
    return excess_risk_bound

def excess_risk_bound_tsybakov(M, δ, n, α):
    """
    Calculate the excess risk bound based on Tsybakov's noise condition.

    Args:
        M (int): Number of classifiers.
        δ (float): Probability threshold.
        n (int): Number of samples.
        α (float): Constant α satisfying Tsybakov's noise condition.

    Returns:
        float: Excess risk bound.
    """
    log_term = math.log(M / δ)
    excess_risk_bound = (log_term) / (n * (2**(-α)))
    return excess_risk_bound

# Example usage:
M = 100  # Number of classifiers
δ = 0.1  # Probability threshold
n = 1000  # Number of samples
γ = 0.1  # Constant γ for Massart's noise condition
α = 0.5  # Constant α for Tsybakov's noise condition

# Calculate excess risk bounds
massart_excess_risk_bound = excess_risk_bound(M, δ, n, γ)
tsybakov_excess_risk_bound = excess_risk_bound_tsybakov(M, δ, n, α)

print("Excess risk bound with Massart's noise condition:", massart_excess_risk_bound)
print("Excess risk bound with Tsybakov's noise condition:", tsybakov_excess_risk_bound)

"""#VAPNIK-CHERVONENKIS (VC) THEORY"""

import numpy as np
from itertools import combinations

def can_shatter(hypothesis_class, points):
    """
    Check if a hypothesis class can shatter a set of points.

    Args:
        hypothesis_class (callable): A function representing the hypothesis class.
        points (list of tuple): List of data points (features).

    Returns:
        bool: True if the hypothesis class can shatter the points, False otherwise.
    """
    n = len(points)
    for subset_size in range(1, n + 1):
        for subset in combinations(points, subset_size):
            subset = list(subset)
            labels = [hypothesis_class(point) for point in subset]
            if len(set(labels)) == subset_size:  # Check if all labelings are possible
                return True
    return False

def estimate_vc_dimension(hypothesis_class, max_points=100, max_trials=1000):
    """
    Estimate the VC dimension of a hypothesis class.

    Args:
        hypothesis_class (callable): A function representing the hypothesis class.
        max_points (int): Maximum number of points to consider.
        max_trials (int): Maximum number of trials to estimate VC dimension.

    Returns:
        int: Estimated VC dimension.
    """
    for d in range(1, max_points + 1):
        points = np.random.rand(d, 2)  # Generate d random points in 2D space
        can_shatter_points = True
        for _ in range(max_trials):
            if not can_shatter(hypothesis_class, points):
                can_shatter_points = False
                break
        if not can_shatter_points:
            return d - 1
    return max_points

# Example of a simple hypothesis class (linear separators)
def linear_separator(x):
    """ A simple linear separator (hypothesis) """
    return 1 if np.dot(x, [1, 1]) > 0 else -1

# Estimate VC dimension of the linear separator hypothesis class
estimated_vc_dimension = estimate_vc_dimension(linear_separator)
print("Estimated VC dimension:", estimated_vc_dimension)

"""#Empirical Measurement of VC Dimension

"""

import numpy as np
from itertools import combinations

def can_shatter(hypothesis_class, points):
    """
    Check if a hypothesis class can shatter a set of points.

    Args:
        hypothesis_class (callable): A function representing the hypothesis class.
        points (list of tuple): List of data points (features).

    Returns:
        bool: True if the hypothesis class can shatter the points, False otherwise.
    """
    n = len(points)
    for subset_size in range(1, n + 1):
        for subset in combinations(points, subset_size):
            subset = list(subset)
            labels = [hypothesis_class(point) for point in subset]
            if len(set(labels)) == subset_size:  # Check if all labelings are possible
                return True
    return False

def estimate_vc_dimension(hypothesis_class, max_points=100, max_trials=1000):
    """
    Estimate the VC dimension of a hypothesis class.

    Args:
        hypothesis_class (callable): A function representing the hypothesis class.
        max_points (int): Maximum number of points to consider.
        max_trials (int): Maximum number of trials to estimate VC dimension.

    Returns:
        int: Estimated VC dimension.
    """
    for d in range(1, max_points + 1):
        points = np.random.rand(d, 2)  # Generate d random points in 2D space
        can_shatter_points = True
        for _ in range(max_trials):
            if not can_shatter(hypothesis_class, points):
                can_shatter_points = False
                break
        if not can_shatter_points:
            return d - 1
    return max_points

# Example of a simple hypothesis class (linear separators)
def linear_separator(x):
    """ A simple linear separator (hypothesis) """
    return 1 if np.dot(x, [1, 1]) > 0 else -1

# Estimate VC dimension of the linear separator hypothesis class
estimated_vc_dimension = estimate_vc_dimension(linear_separator)
print("Estimated VC dimension:", estimated_vc_dimension)

"""#Symmetrization and Rademacher complexity"""

import numpy as np

def symmetrization_inequality(empirical_risk_samples):
    """
    Calculate the upper bound of expected empirical risk using symmetrization inequality.

    Args:
        empirical_risk_samples (numpy.ndarray): Array of empirical risk values over randomized versions.

    Returns:
        float: Upper bound of expected empirical risk.
    """
    return 2 * np.mean(empirical_risk_samples)

def rademacher_complexity(hypothesis_class, data_points):
    """
    Calculate the Rademacher complexity of a hypothesis class.

    Args:
        hypothesis_class (callable): A function representing the hypothesis class.
        data_points (numpy.ndarray): Array of data points (features).

    Returns:
        float: Rademacher complexity.
    """
    n = len(data_points)
    sigma = np.random.choice([-1, 1], size=(n,))  # Generate Rademacher variables for each data point
    randomized_labels = sigma[:, np.newaxis] * data_points  # Randomize labels (broadcasting)
    sup_h = np.array([np.abs(hypothesis_class(x)) for x in randomized_labels])
    return np.mean(sup_h)

# Example usage:
# Define a simple hypothesis class (e.g., linear classifier)
def linear_classifier(x):
    return np.dot(x, [1, -1])  # Example linear classifier

# Generate random data points
n_samples = 100
data_points = np.random.randn(n_samples, 2)

# Calculate Rademacher complexity of the linear classifier
rademacher_comp = rademacher_complexity(linear_classifier, data_points)
print("Rademacher Complexity:", rademacher_comp)

# Generate random empirical risk samples (for symmetrization)
n_samples_empirical_risk = 100
empirical_risk_samples = np.random.rand(n_samples_empirical_risk)  # Example random empirical risk values

# Calculate upper bound of expected empirical risk using symmetrization inequality
expected_empirical_risk_upper_bound = symmetrization_inequality(empirical_risk_samples)
print("Upper bound of Expected Empirical Risk:", expected_empirical_risk_upper_bound)