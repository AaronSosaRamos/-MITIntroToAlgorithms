{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Fast Rates and VC Theory"
      ],
      "metadata": {
        "id": "fmouaeLgESKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noise conditions"
      ],
      "metadata": {
        "id": "4ObNDpuTERaC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weGzmk8oD4ml",
        "outputId": "2e33c9f3-ed87-44a1-e4ed-a8ccfcb845ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excess risk bound with Massart's noise condition: 0.06907755278982136\n",
            "Excess risk bound with Tsybakov's noise condition: 0.009769041201090881\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def excess_risk_bound(M, δ, n, γ):\n",
        "    \"\"\"\n",
        "    Calculate the excess risk bound based on Massart's noise condition.\n",
        "\n",
        "    Args:\n",
        "        M (int): Number of classifiers.\n",
        "        δ (float): Probability threshold.\n",
        "        n (int): Number of samples.\n",
        "        γ (float): Constant γ satisfying Massart's noise condition.\n",
        "\n",
        "    Returns:\n",
        "        float: Excess risk bound.\n",
        "    \"\"\"\n",
        "    log_term = math.log(M / δ)\n",
        "    excess_risk_bound = (log_term) / (γ * n)\n",
        "    return excess_risk_bound\n",
        "\n",
        "def excess_risk_bound_tsybakov(M, δ, n, α):\n",
        "    \"\"\"\n",
        "    Calculate the excess risk bound based on Tsybakov's noise condition.\n",
        "\n",
        "    Args:\n",
        "        M (int): Number of classifiers.\n",
        "        δ (float): Probability threshold.\n",
        "        n (int): Number of samples.\n",
        "        α (float): Constant α satisfying Tsybakov's noise condition.\n",
        "\n",
        "    Returns:\n",
        "        float: Excess risk bound.\n",
        "    \"\"\"\n",
        "    log_term = math.log(M / δ)\n",
        "    excess_risk_bound = (log_term) / (n * (2**(-α)))\n",
        "    return excess_risk_bound\n",
        "\n",
        "# Example usage:\n",
        "M = 100  # Number of classifiers\n",
        "δ = 0.1  # Probability threshold\n",
        "n = 1000  # Number of samples\n",
        "γ = 0.1  # Constant γ for Massart's noise condition\n",
        "α = 0.5  # Constant α for Tsybakov's noise condition\n",
        "\n",
        "# Calculate excess risk bounds\n",
        "massart_excess_risk_bound = excess_risk_bound(M, δ, n, γ)\n",
        "tsybakov_excess_risk_bound = excess_risk_bound_tsybakov(M, δ, n, α)\n",
        "\n",
        "print(\"Excess risk bound with Massart's noise condition:\", massart_excess_risk_bound)\n",
        "print(\"Excess risk bound with Tsybakov's noise condition:\", tsybakov_excess_risk_bound)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VAPNIK-CHERVONENKIS (VC) THEORY"
      ],
      "metadata": {
        "id": "B67z1yVpEZCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "def can_shatter(hypothesis_class, points):\n",
        "    \"\"\"\n",
        "    Check if a hypothesis class can shatter a set of points.\n",
        "\n",
        "    Args:\n",
        "        hypothesis_class (callable): A function representing the hypothesis class.\n",
        "        points (list of tuple): List of data points (features).\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the hypothesis class can shatter the points, False otherwise.\n",
        "    \"\"\"\n",
        "    n = len(points)\n",
        "    for subset_size in range(1, n + 1):\n",
        "        for subset in combinations(points, subset_size):\n",
        "            subset = list(subset)\n",
        "            labels = [hypothesis_class(point) for point in subset]\n",
        "            if len(set(labels)) == subset_size:  # Check if all labelings are possible\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def estimate_vc_dimension(hypothesis_class, max_points=100, max_trials=1000):\n",
        "    \"\"\"\n",
        "    Estimate the VC dimension of a hypothesis class.\n",
        "\n",
        "    Args:\n",
        "        hypothesis_class (callable): A function representing the hypothesis class.\n",
        "        max_points (int): Maximum number of points to consider.\n",
        "        max_trials (int): Maximum number of trials to estimate VC dimension.\n",
        "\n",
        "    Returns:\n",
        "        int: Estimated VC dimension.\n",
        "    \"\"\"\n",
        "    for d in range(1, max_points + 1):\n",
        "        points = np.random.rand(d, 2)  # Generate d random points in 2D space\n",
        "        can_shatter_points = True\n",
        "        for _ in range(max_trials):\n",
        "            if not can_shatter(hypothesis_class, points):\n",
        "                can_shatter_points = False\n",
        "                break\n",
        "        if not can_shatter_points:\n",
        "            return d - 1\n",
        "    return max_points\n",
        "\n",
        "# Example of a simple hypothesis class (linear separators)\n",
        "def linear_separator(x):\n",
        "    \"\"\" A simple linear separator (hypothesis) \"\"\"\n",
        "    return 1 if np.dot(x, [1, 1]) > 0 else -1\n",
        "\n",
        "# Estimate VC dimension of the linear separator hypothesis class\n",
        "estimated_vc_dimension = estimate_vc_dimension(linear_separator)\n",
        "print(\"Estimated VC dimension:\", estimated_vc_dimension)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSK999T7EZ3I",
        "outputId": "30a7fee5-15d5-4044-e900-c10f1d78e947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated VC dimension: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Empirical Measurement of VC Dimension\n"
      ],
      "metadata": {
        "id": "zXsDmt_CFF7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "def can_shatter(hypothesis_class, points):\n",
        "    \"\"\"\n",
        "    Check if a hypothesis class can shatter a set of points.\n",
        "\n",
        "    Args:\n",
        "        hypothesis_class (callable): A function representing the hypothesis class.\n",
        "        points (list of tuple): List of data points (features).\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the hypothesis class can shatter the points, False otherwise.\n",
        "    \"\"\"\n",
        "    n = len(points)\n",
        "    for subset_size in range(1, n + 1):\n",
        "        for subset in combinations(points, subset_size):\n",
        "            subset = list(subset)\n",
        "            labels = [hypothesis_class(point) for point in subset]\n",
        "            if len(set(labels)) == subset_size:  # Check if all labelings are possible\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def estimate_vc_dimension(hypothesis_class, max_points=100, max_trials=1000):\n",
        "    \"\"\"\n",
        "    Estimate the VC dimension of a hypothesis class.\n",
        "\n",
        "    Args:\n",
        "        hypothesis_class (callable): A function representing the hypothesis class.\n",
        "        max_points (int): Maximum number of points to consider.\n",
        "        max_trials (int): Maximum number of trials to estimate VC dimension.\n",
        "\n",
        "    Returns:\n",
        "        int: Estimated VC dimension.\n",
        "    \"\"\"\n",
        "    for d in range(1, max_points + 1):\n",
        "        points = np.random.rand(d, 2)  # Generate d random points in 2D space\n",
        "        can_shatter_points = True\n",
        "        for _ in range(max_trials):\n",
        "            if not can_shatter(hypothesis_class, points):\n",
        "                can_shatter_points = False\n",
        "                break\n",
        "        if not can_shatter_points:\n",
        "            return d - 1\n",
        "    return max_points\n",
        "\n",
        "# Example of a simple hypothesis class (linear separators)\n",
        "def linear_separator(x):\n",
        "    \"\"\" A simple linear separator (hypothesis) \"\"\"\n",
        "    return 1 if np.dot(x, [1, 1]) > 0 else -1\n",
        "\n",
        "# Estimate VC dimension of the linear separator hypothesis class\n",
        "estimated_vc_dimension = estimate_vc_dimension(linear_separator)\n",
        "print(\"Estimated VC dimension:\", estimated_vc_dimension)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m40c8DZVFHO3",
        "outputId": "8cc1d297-3539-4550-b189-1ed954b5aa9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated VC dimension: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Symmetrization and Rademacher complexity"
      ],
      "metadata": {
        "id": "9B7DN6AsIN0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def symmetrization_inequality(empirical_risk_samples):\n",
        "    \"\"\"\n",
        "    Calculate the upper bound of expected empirical risk using symmetrization inequality.\n",
        "\n",
        "    Args:\n",
        "        empirical_risk_samples (numpy.ndarray): Array of empirical risk values over randomized versions.\n",
        "\n",
        "    Returns:\n",
        "        float: Upper bound of expected empirical risk.\n",
        "    \"\"\"\n",
        "    return 2 * np.mean(empirical_risk_samples)\n",
        "\n",
        "def rademacher_complexity(hypothesis_class, data_points):\n",
        "    \"\"\"\n",
        "    Calculate the Rademacher complexity of a hypothesis class.\n",
        "\n",
        "    Args:\n",
        "        hypothesis_class (callable): A function representing the hypothesis class.\n",
        "        data_points (numpy.ndarray): Array of data points (features).\n",
        "\n",
        "    Returns:\n",
        "        float: Rademacher complexity.\n",
        "    \"\"\"\n",
        "    n = len(data_points)\n",
        "    sigma = np.random.choice([-1, 1], size=(n,))  # Generate Rademacher variables for each data point\n",
        "    randomized_labels = sigma[:, np.newaxis] * data_points  # Randomize labels (broadcasting)\n",
        "    sup_h = np.array([np.abs(hypothesis_class(x)) for x in randomized_labels])\n",
        "    return np.mean(sup_h)\n",
        "\n",
        "# Example usage:\n",
        "# Define a simple hypothesis class (e.g., linear classifier)\n",
        "def linear_classifier(x):\n",
        "    return np.dot(x, [1, -1])  # Example linear classifier\n",
        "\n",
        "# Generate random data points\n",
        "n_samples = 100\n",
        "data_points = np.random.randn(n_samples, 2)\n",
        "\n",
        "# Calculate Rademacher complexity of the linear classifier\n",
        "rademacher_comp = rademacher_complexity(linear_classifier, data_points)\n",
        "print(\"Rademacher Complexity:\", rademacher_comp)\n",
        "\n",
        "# Generate random empirical risk samples (for symmetrization)\n",
        "n_samples_empirical_risk = 100\n",
        "empirical_risk_samples = np.random.rand(n_samples_empirical_risk)  # Example random empirical risk values\n",
        "\n",
        "# Calculate upper bound of expected empirical risk using symmetrization inequality\n",
        "expected_empirical_risk_upper_bound = symmetrization_inequality(empirical_risk_samples)\n",
        "print(\"Upper bound of Expected Empirical Risk:\", expected_empirical_risk_upper_bound)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwA_NVfKGyhm",
        "outputId": "b6967ddd-3d2c-4df9-f1df-0d0da9d7538c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rademacher Complexity: 1.0539946745026405\n",
            "Upper bound of Expected Empirical Risk: 1.0346283701049668\n"
          ]
        }
      ]
    }
  ]
}