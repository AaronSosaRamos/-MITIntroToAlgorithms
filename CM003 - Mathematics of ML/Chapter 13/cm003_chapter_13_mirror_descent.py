# -*- coding: utf-8 -*-
"""CM003 - Chapter 13 - Mirror Descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BUE6a2S3TMzf7Gb1sHmEQWdrnBZiJDTk

#Mirror Descent
"""

import numpy as np

def mirror_descent(gradient_func, mirror_map_func, initial_point, num_iterations, step_size):
    """
    Mirror Descent Algorithm for convex optimization.

    Args:
        gradient_func (function): Function to compute the gradient of the objective.
        mirror_map_func (function): Mirror map function used in the algorithm.
        initial_point (numpy.ndarray): Initial point for optimization.
        num_iterations (int): Number of iterations.
        step_size (float): Step size or learning rate.

    Returns:
        numpy.ndarray: Optimal point found by the algorithm.
    """

    # Initialize the current point
    current_point = initial_point.copy()

    # Perform Mirror Descent iterations
    for t in range(1, num_iterations + 1):
        # Compute gradient at the current point
        gradient = gradient_func(current_point)

        # Compute the mirror point using the mirror map
        mirror_point = mirror_map_func(current_point - step_size * gradient)

        # Update the current point using the mirror point
        current_point = current_point - step_size * gradient_func(mirror_point)

    return current_point

# Example usage:
if __name__ == "__main__":
    # Define the objective function and its gradient
    def quadratic_objective(x):
        return np.linalg.norm(x)**2

    def gradient_of_quadratic_objective(x):
        return 2 * x

    # Define the mirror map (e.g., L2 norm squared)
    def l2_squared_mirror_map(x):
        return 0.5 * np.linalg.norm(x)**2

    # Set up parameters
    initial_point = np.array([1.0, 1.0])  # Initial point
    num_iterations = 100  # Number of iterations
    step_size = 0.1  # Step size

    # Run Mirror Descent
    result = mirror_descent(gradient_of_quadratic_objective, l2_squared_mirror_map, initial_point, num_iterations, step_size)

    print("Optimal point:", result)
    print("Objective value at optimal point:", quadratic_objective(result))

"""#Euclidean Setup:"""

import numpy as np

def mirror_descent_euclidean(gradient_func, initial_point, num_iterations, step_size):
    """
    Mirror Descent Algorithm for convex optimization in Euclidean setup.

    Args:
        gradient_func (function): Function to compute the gradient of the objective.
        initial_point (numpy.ndarray): Initial point for optimization.
        num_iterations (int): Number of iterations.
        step_size (float): Step size or learning rate.

    Returns:
        numpy.ndarray: Optimal point found by the algorithm.
    """

    # Initialize the current point
    current_point = initial_point.copy()

    # Perform Mirror Descent iterations
    for t in range(1, num_iterations + 1):
        # Compute gradient at the current point
        gradient = gradient_func(current_point)

        # Update the current point using the gradient and step size
        current_point = current_point - step_size * gradient

    return current_point

# Example usage:
if __name__ == "__main__":
    # Define the objective function and its gradient (example: quadratic function)
    def quadratic_objective(x):
        return np.linalg.norm(x)**2

    def gradient_of_quadratic_objective(x):
        return 2 * x

    # Set up parameters
    initial_point = np.array([1.0, 1.0])  # Initial point
    num_iterations = 100  # Number of iterations
    step_size = 0.1  # Step size

    # Run Mirror Descent in Euclidean setup
    result = mirror_descent_euclidean(gradient_of_quadratic_objective, initial_point, num_iterations, step_size)

    print("Optimal point:", result)
    print("Objective value at optimal point:", quadratic_objective(result))

"""#â„“1 Setup:"""

import numpy as np

def mirror_descent_l1(gradient_func, initial_point, num_iterations, step_size, l1_reg):
    """
    Mirror Descent Algorithm for convex optimization with L1 regularization.

    Args:
        gradient_func (function): Function to compute the gradient of the objective.
        initial_point (numpy.ndarray): Initial point for optimization.
        num_iterations (int): Number of iterations.
        step_size (float): Step size or learning rate.
        l1_reg (float): Strength of L1 regularization.

    Returns:
        numpy.ndarray: Optimal point found by the algorithm.
    """

    # Mirror map using L-infinity norm projection
    def mirror_map(x, step_size):
        return np.sign(x) * np.maximum(0, np.abs(x) - step_size * l1_reg)

    # Initialize the current point
    current_point = initial_point.copy()

    # Perform Mirror Descent iterations
    for _ in range(num_iterations):
        # Compute gradient at the current point
        gradient = gradient_func(current_point)

        # Update the current point using Mirror Descent update rule
        current_point = mirror_map(current_point - step_size * gradient, step_size)

    return current_point

# Example usage:
if __name__ == "__main__":
    # Define the objective function and its gradient (quadratic function with L1 regularization)
    def objective_function(x, l1_reg):
        return np.linalg.norm(x)**2 + l1_reg * np.linalg.norm(x, ord=1)

    def gradient_of_objective(x):
        return 2 * x + l1_reg * np.sign(x)

    # Set up parameters
    initial_point = np.array([1.0, 1.0])  # Initial point
    num_iterations = 100  # Number of iterations
    step_size = 0.1  # Step size (learning rate)
    l1_reg = 0.1  # Strength of L1 regularization

    # Run Mirror Descent with L1 regularization
    result = mirror_descent_l1(gradient_of_objective, initial_point, num_iterations, step_size, l1_reg)

    # Compute objective value at optimal point
    optimal_value = objective_function(result, l1_reg)

    print("Optimal point:", result)
    print("Objective value at optimal point:", optimal_value)

import numpy as np

def mirror_descent_boosting(loss_func_gradient, initial_weights, num_iterations, step_size):
    """
    Mirror Descent Algorithm for boosting.

    Args:
        loss_func_gradient (function): Function to compute the gradient of the loss function.
        initial_weights (numpy.ndarray): Initial weights for the weak learners.
        num_iterations (int): Number of boosting iterations.
        step_size (float): Step size or learning rate.

    Returns:
        numpy.ndarray: Final weights for the weak learners.
    """

    # Initialize weights for weak learners
    current_weights = initial_weights.copy()

    # Perform Mirror Descent iterations for boosting
    for t in range(num_iterations):
        # Compute the gradient of the loss function with respect to current weights
        gradient = loss_func_gradient(current_weights)

        # Update the current weights using Mirror Descent update rule
        current_weights = current_weights * np.exp(-step_size * gradient)

        # Normalize weights to ensure they form a probability distribution
        current_weights /= np.sum(current_weights)

    return current_weights

# Example usage:
if __name__ == "__main__":
    # Define the loss function gradient (example: exponential loss for AdaBoost)
    def exponential_loss_gradient(weights):
        # Compute the gradient of exponential loss with respect to weights
        return np.random.rand(len(weights))  # Placeholder for actual gradient computation

    # Set up parameters
    num_weak_learners = 10  # Number of weak learners (e.g., decision stumps)
    initial_weights = np.ones(num_weak_learners) / num_weak_learners  # Initial uniform weights
    num_iterations = 100  # Number of boosting iterations
    step_size = 0.1  # Step size (learning rate)

    # Run Mirror Descent for boosting
    final_weights = mirror_descent_boosting(exponential_loss_gradient, initial_weights, num_iterations, step_size)

    print("Final weights for weak learners:", final_weights)

"""#Other Potential Functions:

"""

import numpy as np

def mirror_descent(gradient_func, mirror_map_func, initial_point, num_iterations, step_size):
    """
    Mirror Descent Algorithm for convex optimization with custom mirror map.

    Args:
        gradient_func (function): Function to compute the gradient of the objective.
        mirror_map_func (function): Mirror map function used in the Mirror Descent.
        initial_point (numpy.ndarray): Initial point for optimization.
        num_iterations (int): Number of iterations.
        step_size (float): Step size or learning rate.

    Returns:
        numpy.ndarray: Optimal point found by the algorithm.
    """

    # Initialize the current point
    current_point = initial_point.copy()

    # Perform Mirror Descent iterations
    for _ in range(num_iterations):
        # Compute gradient at the current point
        gradient = gradient_func(current_point)

        # Update the current point using Mirror Descent with custom mirror map
        current_point = mirror_map_func(current_point, gradient, step_size)

    return current_point

def l2_squared_mirror_map(x, gradient, step_size):
    return x - step_size * gradient

def l1_mirror_map(x, gradient, step_size):
    return np.sign(x) * np.maximum(0, np.abs(x) - step_size)

def clipped_exp(x, gradient, step_size):
    return np.clip(x - step_size * gradient, -10, 10)  # Clip to avoid overflow

# Example usage with different mirror maps:
if __name__ == "__main__":
    # Define the objective function and its gradient (example: quadratic function)
    def quadratic_objective(x):
        return np.linalg.norm(x)**2

    def gradient_of_quadratic_objective(x):
        return 2 * x

    # Set up parameters
    initial_point = np.array([1.0, 1.0])  # Initial point
    num_iterations = 100  # Number of iterations
    step_size = 0.1  # Step size (learning rate)

    # Run Mirror Descent with different mirror maps
    # Mirror Descent with L2-squared mirror map
    result_l2 = mirror_descent(gradient_of_quadratic_objective, l2_squared_mirror_map, initial_point, num_iterations, step_size)

    # Mirror Descent with L1 mirror map (for L1 regularization)
    result_l1 = mirror_descent(gradient_of_quadratic_objective, l1_mirror_map, initial_point, num_iterations, step_size)

    # Mirror Descent with clipped exponential mirror map
    result_exp = mirror_descent(gradient_of_quadratic_objective, clipped_exp, initial_point, num_iterations, step_size)

    # Evaluate and print results
    print("Optimal point with L2-squared mirror map:", result_l2)
    print("Objective value at optimal point (L2-squared mirror map):", quadratic_objective(result_l2))

    print("Optimal point with L1 mirror map:", result_l1)
    print("Objective value at optimal point (L1 mirror map):", quadratic_objective(result_l1))

    print("Optimal point with clipped exponential mirror map:", result_exp)
    print("Objective value at optimal point (clipped exponential mirror map):", quadratic_objective(result_exp))