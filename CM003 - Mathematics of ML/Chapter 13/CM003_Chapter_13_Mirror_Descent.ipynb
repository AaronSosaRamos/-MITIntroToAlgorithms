{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Mirror Descent"
      ],
      "metadata": {
        "id": "5mxnEs7GWSIb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-6b8f9nWOkx",
        "outputId": "c2b4c011-b210-4264-e831-04865c52dad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal point: [0.07068353 0.07068353]\n",
            "Objective value at optimal point: 0.009992323888524907\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def mirror_descent(gradient_func, mirror_map_func, initial_point, num_iterations, step_size):\n",
        "    \"\"\"\n",
        "    Mirror Descent Algorithm for convex optimization.\n",
        "\n",
        "    Args:\n",
        "        gradient_func (function): Function to compute the gradient of the objective.\n",
        "        mirror_map_func (function): Mirror map function used in the algorithm.\n",
        "        initial_point (numpy.ndarray): Initial point for optimization.\n",
        "        num_iterations (int): Number of iterations.\n",
        "        step_size (float): Step size or learning rate.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Optimal point found by the algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the current point\n",
        "    current_point = initial_point.copy()\n",
        "\n",
        "    # Perform Mirror Descent iterations\n",
        "    for t in range(1, num_iterations + 1):\n",
        "        # Compute gradient at the current point\n",
        "        gradient = gradient_func(current_point)\n",
        "\n",
        "        # Compute the mirror point using the mirror map\n",
        "        mirror_point = mirror_map_func(current_point - step_size * gradient)\n",
        "\n",
        "        # Update the current point using the mirror point\n",
        "        current_point = current_point - step_size * gradient_func(mirror_point)\n",
        "\n",
        "    return current_point\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the objective function and its gradient\n",
        "    def quadratic_objective(x):\n",
        "        return np.linalg.norm(x)**2\n",
        "\n",
        "    def gradient_of_quadratic_objective(x):\n",
        "        return 2 * x\n",
        "\n",
        "    # Define the mirror map (e.g., L2 norm squared)\n",
        "    def l2_squared_mirror_map(x):\n",
        "        return 0.5 * np.linalg.norm(x)**2\n",
        "\n",
        "    # Set up parameters\n",
        "    initial_point = np.array([1.0, 1.0])  # Initial point\n",
        "    num_iterations = 100  # Number of iterations\n",
        "    step_size = 0.1  # Step size\n",
        "\n",
        "    # Run Mirror Descent\n",
        "    result = mirror_descent(gradient_of_quadratic_objective, l2_squared_mirror_map, initial_point, num_iterations, step_size)\n",
        "\n",
        "    print(\"Optimal point:\", result)\n",
        "    print(\"Objective value at optimal point:\", quadratic_objective(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Euclidean Setup:"
      ],
      "metadata": {
        "id": "RXLu7ckmWihr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mirror_descent_euclidean(gradient_func, initial_point, num_iterations, step_size):\n",
        "    \"\"\"\n",
        "    Mirror Descent Algorithm for convex optimization in Euclidean setup.\n",
        "\n",
        "    Args:\n",
        "        gradient_func (function): Function to compute the gradient of the objective.\n",
        "        initial_point (numpy.ndarray): Initial point for optimization.\n",
        "        num_iterations (int): Number of iterations.\n",
        "        step_size (float): Step size or learning rate.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Optimal point found by the algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the current point\n",
        "    current_point = initial_point.copy()\n",
        "\n",
        "    # Perform Mirror Descent iterations\n",
        "    for t in range(1, num_iterations + 1):\n",
        "        # Compute gradient at the current point\n",
        "        gradient = gradient_func(current_point)\n",
        "\n",
        "        # Update the current point using the gradient and step size\n",
        "        current_point = current_point - step_size * gradient\n",
        "\n",
        "    return current_point\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the objective function and its gradient (example: quadratic function)\n",
        "    def quadratic_objective(x):\n",
        "        return np.linalg.norm(x)**2\n",
        "\n",
        "    def gradient_of_quadratic_objective(x):\n",
        "        return 2 * x\n",
        "\n",
        "    # Set up parameters\n",
        "    initial_point = np.array([1.0, 1.0])  # Initial point\n",
        "    num_iterations = 100  # Number of iterations\n",
        "    step_size = 0.1  # Step size\n",
        "\n",
        "    # Run Mirror Descent in Euclidean setup\n",
        "    result = mirror_descent_euclidean(gradient_of_quadratic_objective, initial_point, num_iterations, step_size)\n",
        "\n",
        "    print(\"Optimal point:\", result)\n",
        "    print(\"Objective value at optimal point:\", quadratic_objective(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk8sdT8cWhjL",
        "outputId": "21fc62ef-c916-4636-cc3d-7e414ef26510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal point: [2.03703598e-10 2.03703598e-10]\n",
            "Objective value at optimal point: 8.299031137762e-20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#â„“1 Setup:"
      ],
      "metadata": {
        "id": "rD2_XuAKWqBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mirror_descent_l1(gradient_func, initial_point, num_iterations, step_size, l1_reg):\n",
        "    \"\"\"\n",
        "    Mirror Descent Algorithm for convex optimization with L1 regularization.\n",
        "\n",
        "    Args:\n",
        "        gradient_func (function): Function to compute the gradient of the objective.\n",
        "        initial_point (numpy.ndarray): Initial point for optimization.\n",
        "        num_iterations (int): Number of iterations.\n",
        "        step_size (float): Step size or learning rate.\n",
        "        l1_reg (float): Strength of L1 regularization.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Optimal point found by the algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    # Mirror map using L-infinity norm projection\n",
        "    def mirror_map(x, step_size):\n",
        "        return np.sign(x) * np.maximum(0, np.abs(x) - step_size * l1_reg)\n",
        "\n",
        "    # Initialize the current point\n",
        "    current_point = initial_point.copy()\n",
        "\n",
        "    # Perform Mirror Descent iterations\n",
        "    for _ in range(num_iterations):\n",
        "        # Compute gradient at the current point\n",
        "        gradient = gradient_func(current_point)\n",
        "\n",
        "        # Update the current point using Mirror Descent update rule\n",
        "        current_point = mirror_map(current_point - step_size * gradient, step_size)\n",
        "\n",
        "    return current_point\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the objective function and its gradient (quadratic function with L1 regularization)\n",
        "    def objective_function(x, l1_reg):\n",
        "        return np.linalg.norm(x)**2 + l1_reg * np.linalg.norm(x, ord=1)\n",
        "\n",
        "    def gradient_of_objective(x):\n",
        "        return 2 * x + l1_reg * np.sign(x)\n",
        "\n",
        "    # Set up parameters\n",
        "    initial_point = np.array([1.0, 1.0])  # Initial point\n",
        "    num_iterations = 100  # Number of iterations\n",
        "    step_size = 0.1  # Step size (learning rate)\n",
        "    l1_reg = 0.1  # Strength of L1 regularization\n",
        "\n",
        "    # Run Mirror Descent with L1 regularization\n",
        "    result = mirror_descent_l1(gradient_of_objective, initial_point, num_iterations, step_size, l1_reg)\n",
        "\n",
        "    # Compute objective value at optimal point\n",
        "    optimal_value = objective_function(result, l1_reg)\n",
        "\n",
        "    print(\"Optimal point:\", result)\n",
        "    print(\"Objective value at optimal point:\", optimal_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUuFxloxWrBV",
        "outputId": "e81efac0-4836-4ad8-fa3d-f725bd4fd287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal point: [0. 0.]\n",
            "Objective value at optimal point: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mirror_descent_boosting(loss_func_gradient, initial_weights, num_iterations, step_size):\n",
        "    \"\"\"\n",
        "    Mirror Descent Algorithm for boosting.\n",
        "\n",
        "    Args:\n",
        "        loss_func_gradient (function): Function to compute the gradient of the loss function.\n",
        "        initial_weights (numpy.ndarray): Initial weights for the weak learners.\n",
        "        num_iterations (int): Number of boosting iterations.\n",
        "        step_size (float): Step size or learning rate.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Final weights for the weak learners.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize weights for weak learners\n",
        "    current_weights = initial_weights.copy()\n",
        "\n",
        "    # Perform Mirror Descent iterations for boosting\n",
        "    for t in range(num_iterations):\n",
        "        # Compute the gradient of the loss function with respect to current weights\n",
        "        gradient = loss_func_gradient(current_weights)\n",
        "\n",
        "        # Update the current weights using Mirror Descent update rule\n",
        "        current_weights = current_weights * np.exp(-step_size * gradient)\n",
        "\n",
        "        # Normalize weights to ensure they form a probability distribution\n",
        "        current_weights /= np.sum(current_weights)\n",
        "\n",
        "    return current_weights\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the loss function gradient (example: exponential loss for AdaBoost)\n",
        "    def exponential_loss_gradient(weights):\n",
        "        # Compute the gradient of exponential loss with respect to weights\n",
        "        return np.random.rand(len(weights))  # Placeholder for actual gradient computation\n",
        "\n",
        "    # Set up parameters\n",
        "    num_weak_learners = 10  # Number of weak learners (e.g., decision stumps)\n",
        "    initial_weights = np.ones(num_weak_learners) / num_weak_learners  # Initial uniform weights\n",
        "    num_iterations = 100  # Number of boosting iterations\n",
        "    step_size = 0.1  # Step size (learning rate)\n",
        "\n",
        "    # Run Mirror Descent for boosting\n",
        "    final_weights = mirror_descent_boosting(exponential_loss_gradient, initial_weights, num_iterations, step_size)\n",
        "\n",
        "    print(\"Final weights for weak learners:\", final_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JuXzjOVW2L-",
        "outputId": "1cddb460-9f9a-40e5-e891-7ac765ce74f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights for weak learners: [0.12222487 0.09131768 0.10649524 0.05969399 0.07634369 0.1358873\n",
            " 0.12176933 0.08990098 0.10010767 0.09625924]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Other Potential Functions:\n"
      ],
      "metadata": {
        "id": "tiH-xLfJXL-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mirror_descent(gradient_func, mirror_map_func, initial_point, num_iterations, step_size):\n",
        "    \"\"\"\n",
        "    Mirror Descent Algorithm for convex optimization with custom mirror map.\n",
        "\n",
        "    Args:\n",
        "        gradient_func (function): Function to compute the gradient of the objective.\n",
        "        mirror_map_func (function): Mirror map function used in the Mirror Descent.\n",
        "        initial_point (numpy.ndarray): Initial point for optimization.\n",
        "        num_iterations (int): Number of iterations.\n",
        "        step_size (float): Step size or learning rate.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Optimal point found by the algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the current point\n",
        "    current_point = initial_point.copy()\n",
        "\n",
        "    # Perform Mirror Descent iterations\n",
        "    for _ in range(num_iterations):\n",
        "        # Compute gradient at the current point\n",
        "        gradient = gradient_func(current_point)\n",
        "\n",
        "        # Update the current point using Mirror Descent with custom mirror map\n",
        "        current_point = mirror_map_func(current_point, gradient, step_size)\n",
        "\n",
        "    return current_point\n",
        "\n",
        "def l2_squared_mirror_map(x, gradient, step_size):\n",
        "    return x - step_size * gradient\n",
        "\n",
        "def l1_mirror_map(x, gradient, step_size):\n",
        "    return np.sign(x) * np.maximum(0, np.abs(x) - step_size)\n",
        "\n",
        "def clipped_exp(x, gradient, step_size):\n",
        "    return np.clip(x - step_size * gradient, -10, 10)  # Clip to avoid overflow\n",
        "\n",
        "# Example usage with different mirror maps:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the objective function and its gradient (example: quadratic function)\n",
        "    def quadratic_objective(x):\n",
        "        return np.linalg.norm(x)**2\n",
        "\n",
        "    def gradient_of_quadratic_objective(x):\n",
        "        return 2 * x\n",
        "\n",
        "    # Set up parameters\n",
        "    initial_point = np.array([1.0, 1.0])  # Initial point\n",
        "    num_iterations = 100  # Number of iterations\n",
        "    step_size = 0.1  # Step size (learning rate)\n",
        "\n",
        "    # Run Mirror Descent with different mirror maps\n",
        "    # Mirror Descent with L2-squared mirror map\n",
        "    result_l2 = mirror_descent(gradient_of_quadratic_objective, l2_squared_mirror_map, initial_point, num_iterations, step_size)\n",
        "\n",
        "    # Mirror Descent with L1 mirror map (for L1 regularization)\n",
        "    result_l1 = mirror_descent(gradient_of_quadratic_objective, l1_mirror_map, initial_point, num_iterations, step_size)\n",
        "\n",
        "    # Mirror Descent with clipped exponential mirror map\n",
        "    result_exp = mirror_descent(gradient_of_quadratic_objective, clipped_exp, initial_point, num_iterations, step_size)\n",
        "\n",
        "    # Evaluate and print results\n",
        "    print(\"Optimal point with L2-squared mirror map:\", result_l2)\n",
        "    print(\"Objective value at optimal point (L2-squared mirror map):\", quadratic_objective(result_l2))\n",
        "\n",
        "    print(\"Optimal point with L1 mirror map:\", result_l1)\n",
        "    print(\"Objective value at optimal point (L1 mirror map):\", quadratic_objective(result_l1))\n",
        "\n",
        "    print(\"Optimal point with clipped exponential mirror map:\", result_exp)\n",
        "    print(\"Objective value at optimal point (clipped exponential mirror map):\", quadratic_objective(result_exp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xtjJMOQXMfa",
        "outputId": "1b66047c-1470-4f68-9cde-bc517220190c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal point with L2-squared mirror map: [2.03703598e-10 2.03703598e-10]\n",
            "Objective value at optimal point (L2-squared mirror map): 8.299031137762e-20\n",
            "Optimal point with L1 mirror map: [0. 0.]\n",
            "Objective value at optimal point (L1 mirror map): 0.0\n",
            "Optimal point with clipped exponential mirror map: [2.03703598e-10 2.03703598e-10]\n",
            "Objective value at optimal point (clipped exponential mirror map): 8.299031137762e-20\n"
          ]
        }
      ]
    }
  ]
}